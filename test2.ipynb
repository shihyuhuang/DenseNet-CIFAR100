{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDaVVUNSEW4Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wxdwi7bDf7M9"
   },
   "source": [
    "# 7/25更新\n",
    "- 可以使用pretrain weight嗎? Ans: No\n",
    "- 一定要使用Resnet18 model嗎? Ans: 不一定，可以使用其他model\n",
    "\n",
    "# 常見問題：\n",
    "- 沒有GPU或其他環境問題：請你使用 colab\n",
    "- Train 很慢：請你檢查 get_device() 看有沒有成功使用到 GPU\n",
    "- 被 colab 限制 GPU 用量：好像使用超過12小時會被 google 限制用量，\n",
    "請你開其他 google 帳號，或是等個幾小時再繼續\n",
    "- dimension 對不起來：請你檢查 tensor.shape\n",
    "- dataset permission denied：google 有流量限制，請你嘗試其他下載連結"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VU_bea_h8Yf"
   },
   "source": [
    "# Install & import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SvXSh2mXVZuv",
    "outputId": "01be9bc2-825c-4f64-99f7-8a89273c5132"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtoolbox in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (0.1.5)\n",
      "Requirement already satisfied: tqdm in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from torchtoolbox) (4.62.0)\n",
      "Requirement already satisfied: pyarrow in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from torchtoolbox) (5.0.0)\n",
      "Requirement already satisfied: scipy in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from torchtoolbox) (1.5.4)\n",
      "Requirement already satisfied: scikit-learn in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from torchtoolbox) (0.24.2)\n",
      "Requirement already satisfied: lmdb in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from torchtoolbox) (1.2.1)\n",
      "Requirement already satisfied: opencv-python in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from torchtoolbox) (4.5.3.56)\n",
      "Requirement already satisfied: six in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from torchtoolbox) (1.16.0)\n",
      "Requirement already satisfied: numpy in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from torchtoolbox) (1.19.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from scikit-learn->torchtoolbox) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from scikit-learn->torchtoolbox) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtoolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "onvE_O-BiBWu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import tarfile\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder, CIFAR100\n",
    "from torchtoolbox.tools import summary\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams['figure.facecolor'] = '#ffffff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zxfwwt_Hhhy0"
   },
   "source": [
    "# Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbF56ImQhWKn",
    "outputId": "d492cac4-730c-498e-ecdf-42eaa3cb06a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Mon Aug  2 22:29:07 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n",
      "|  0%   35C    P8    11W / 280W |   2251MiB / 11176MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      7373      C   ...hong/anaconda3/bin/python     2249MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWDyhi0afvQI"
   },
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2WIagHuQgxE",
    "outputId": "8fcebb6a-a787-4a4b-81f8-cdec20892ef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opendatasets\n",
      "  Downloading opendatasets-0.1.20-py3-none-any.whl (14 kB)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from opendatasets) (4.62.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from click->opendatasets) (3.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from importlib-metadata->click->opendatasets) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from importlib-metadata->click->opendatasets) (3.10.0.0)\n",
      "Requirement already satisfied: six>=1.10 in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from kaggle->opendatasets) (1.16.0)\n",
      "Requirement already satisfied: certifi in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from kaggle->opendatasets) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil in /home/U111syhuang/anaconda3/envs/torch/lib/python3.6/site-packages (from kaggle->opendatasets) (2.8.2)\n",
      "Collecting requests\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-slugify\n",
      "  Downloading python_slugify-5.0.2-py2.py3-none-any.whl (6.7 kB)\n",
      "Collecting urllib3\n",
      "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.2-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.4-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73053 sha256=f67d4854d7a636eaae79afb6a0888c88d6cb7f5453912049a48ec80bf3398920\n",
      "  Stored in directory: /home/U111syhuang/.cache/pip/wheels/77/47/e4/44a4ba1b7dfd53faaa35f59f1175e123b213ff401a8a56876b\n",
      "Successfully built kaggle\n",
      "Installing collected packages: urllib3, text-unidecode, idna, charset-normalizer, requests, python-slugify, kaggle, click, opendatasets\n",
      "Successfully installed charset-normalizer-2.0.4 click-8.0.1 idna-3.2 kaggle-1.5.12 opendatasets-0.1.20 python-slugify-5.0.2 requests-2.26.0 text-unidecode-1.3 urllib3-1.26.6\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9YLruq3Qkqt",
    "outputId": "47fe8f3f-f420-4931-e61e-ad6fb42f2815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username: shihyuhuang\n",
      "Your Kaggle Key: ········\n",
      "Downloading 2021-ai-training.zip to ./2021-ai-training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140M/140M [00:07<00:00, 19.6MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting archive ./2021-ai-training/2021-ai-training.zip to ./2021-ai-training\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "od.download('https://www.kaggle.com/c/2021-ai-training/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Hrzny0pis-n"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nl1GrQ_fQnHz",
    "outputId": "72d4f66e-6b9c-4340-8d2b-6c64c1e3e41f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "data_dir = './2021-ai-training/CIFAR100'\n",
    "classes = os.listdir(data_dir + \"/TRAIN\")\n",
    "print(classes)\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EmRWOIGAQw26"
   },
   "outputs": [],
   "source": [
    "# Data transforms (normalization & data augmentation)\n",
    "normalize = tt.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "train_tfms = tt.Compose([\n",
    "        tt.RandomCrop(32, padding=4),\n",
    "        tt.RandomHorizontalFlip(),\n",
    "        tt.ToTensor(),\n",
    "        normalize,\n",
    "])\n",
    "\n",
    "valid_tfms = tt.Compose([\n",
    "        tt.ToTensor(),\n",
    "        normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "le19U-sEQ1DI"
   },
   "outputs": [],
   "source": [
    "# PyTorch datasets\n",
    "train_ds = ImageFolder(root=data_dir+\"/TRAIN\",transform=train_tfms)\n",
    "valid_ds = ImageFolder(root=data_dir+\"/TEST\",transform=valid_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FwCoTdSSu17h"
   },
   "outputs": [],
   "source": [
    "# PyTorch data loaders\n",
    "BATCH_SIZE=64\n",
    "train_dl = DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=True, num_workers=2)\n",
    "valid_dl = DataLoader(valid_ds,batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WscSxO5sx4Kx"
   },
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ShFAAMhtx3sG"
   },
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2h1It5lkY4p"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "7P96I9DBlAdR"
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    if epoch < 75:\n",
    "        lrr = lr\n",
    "    elif epoch < 112:\n",
    "        lrr = lr * 0.1\n",
    "    else:\n",
    "        lrr = lr * 0.01\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lrr\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self,batch):\n",
    "        images,labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out,labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch):\n",
    "        images,labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out,labels)\n",
    "        acc = accuracy(out,labels)\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "EKoC39FKRG5Q"
   },
   "outputs": [],
   "source": [
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                        growth_rate, kernel_size=1, stride=1, bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super(_DenseLayer, self).forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet_Cifar(ImageClassificationBase):\n",
    "    def __init__(self, growth_rate=12, block_config=(16, 16, 16),\n",
    "                 num_init_features=24, bn_size=4, drop_rate=0, num_classes=10):\n",
    "\n",
    "        super(DenseNet_Cifar, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
    "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "        # initialize conv and bn parameters\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.avg_pool2d(out, kernel_size=8, stride=1).view(features.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def densenet_BC_cifar(depth, k, **kwargs):\n",
    "    N = (depth - 4) // 6\n",
    "    model = DenseNet_Cifar(growth_rate=k, block_config=[N, N, N], num_init_features=2*k, **kwargs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "8_gJCKI6RI6S"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "model = densenet_BC_cifar(depth=100, k=12, num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "D9KSsamal8qn",
    "outputId": "bb9dc18a-d418-4a95-ee49-b6a518c89d13"
   },
   "outputs": [],
   "source": [
    "model = to_device(model,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhBKbSoHlwD5"
   },
   "source": [
    "# Set Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "4RRcofFsRgwN"
   },
   "outputs": [],
   "source": [
    "epochs=150\n",
    "lr=0.1\n",
    "momentum=0.9\n",
    "weight_decay=1e-4\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEWNYsSImNhA"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "m84S0Bk-mSt2"
   },
   "outputs": [],
   "source": [
    "def Train (epochs,train_dl,valid_dl,model,optimizer):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    history = []\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        lrs = []\n",
    "        \n",
    "        for batch in train_dl:\n",
    "            loss = model.training_step(batch)\n",
    "            train_loss.append(loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lrs.append(get_lr(optimizer))\n",
    "\n",
    "    \n",
    "        result = evaluate(model,valid_dl)\n",
    "        result[\"train_loss\"] = torch.stack(train_loss).mean().item()\n",
    "        result[\"lrs\"] = lrs\n",
    "        \n",
    "        model.epoch_end(epoch,result)\n",
    "        history.append(result)\n",
    "        \n",
    "    return history\n",
    "            \n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model,valid_dl):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in valid_dl]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "uNEAlOh1RkPS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.10000, train_loss: 3.8632, val_loss: 3.5203, val_acc: 0.1678\n",
      "Epoch [1], last_lr: 0.10000, train_loss: 3.0568, val_loss: 2.8238, val_acc: 0.2847\n",
      "Epoch [2], last_lr: 0.10000, train_loss: 2.4879, val_loss: 2.7379, val_acc: 0.3258\n",
      "Epoch [3], last_lr: 0.10000, train_loss: 2.1495, val_loss: 2.7143, val_acc: 0.3441\n",
      "Epoch [4], last_lr: 0.10000, train_loss: 1.9223, val_loss: 2.0449, val_acc: 0.4613\n",
      "Epoch [5], last_lr: 0.10000, train_loss: 1.7640, val_loss: 1.8641, val_acc: 0.4990\n",
      "Epoch [6], last_lr: 0.10000, train_loss: 1.6384, val_loss: 1.8818, val_acc: 0.5078\n",
      "Epoch [7], last_lr: 0.10000, train_loss: 1.5422, val_loss: 1.6739, val_acc: 0.5461\n",
      "Epoch [8], last_lr: 0.10000, train_loss: 1.4631, val_loss: 1.7907, val_acc: 0.5355\n",
      "Epoch [9], last_lr: 0.10000, train_loss: 1.3990, val_loss: 1.6894, val_acc: 0.5468\n",
      "Epoch [10], last_lr: 0.10000, train_loss: 1.3463, val_loss: 1.5426, val_acc: 0.5781\n",
      "Epoch [11], last_lr: 0.10000, train_loss: 1.2975, val_loss: 1.5565, val_acc: 0.5853\n",
      "Epoch [12], last_lr: 0.10000, train_loss: 1.2563, val_loss: 1.7054, val_acc: 0.5592\n",
      "Epoch [13], last_lr: 0.10000, train_loss: 1.2227, val_loss: 1.7730, val_acc: 0.5446\n",
      "Epoch [14], last_lr: 0.10000, train_loss: 1.1784, val_loss: 1.5325, val_acc: 0.5922\n",
      "Epoch [15], last_lr: 0.10000, train_loss: 1.1620, val_loss: 1.5376, val_acc: 0.5954\n",
      "Epoch [16], last_lr: 0.10000, train_loss: 1.1315, val_loss: 1.5247, val_acc: 0.5920\n",
      "Epoch [17], last_lr: 0.10000, train_loss: 1.1044, val_loss: 1.5932, val_acc: 0.5890\n",
      "Epoch [18], last_lr: 0.10000, train_loss: 1.0868, val_loss: 1.4763, val_acc: 0.6063\n",
      "Epoch [19], last_lr: 0.10000, train_loss: 1.0644, val_loss: 1.4233, val_acc: 0.6232\n",
      "Epoch [20], last_lr: 0.10000, train_loss: 1.0536, val_loss: 1.5886, val_acc: 0.5880\n",
      "Epoch [21], last_lr: 0.10000, train_loss: 1.0351, val_loss: 1.5473, val_acc: 0.5973\n",
      "Epoch [22], last_lr: 0.10000, train_loss: 1.0276, val_loss: 1.5183, val_acc: 0.5965\n",
      "Epoch [23], last_lr: 0.10000, train_loss: 1.0025, val_loss: 1.3852, val_acc: 0.6255\n",
      "Epoch [24], last_lr: 0.10000, train_loss: 0.9912, val_loss: 1.4209, val_acc: 0.6242\n",
      "Epoch [25], last_lr: 0.10000, train_loss: 0.9780, val_loss: 1.5014, val_acc: 0.6090\n",
      "Epoch [26], last_lr: 0.10000, train_loss: 0.9634, val_loss: 1.4100, val_acc: 0.6256\n",
      "Epoch [27], last_lr: 0.10000, train_loss: 0.9614, val_loss: 1.5218, val_acc: 0.6115\n",
      "Epoch [28], last_lr: 0.10000, train_loss: 0.9473, val_loss: 1.4151, val_acc: 0.6261\n",
      "Epoch [29], last_lr: 0.10000, train_loss: 0.9323, val_loss: 1.4697, val_acc: 0.6104\n",
      "Epoch [30], last_lr: 0.10000, train_loss: 0.9331, val_loss: 1.3348, val_acc: 0.6355\n",
      "Epoch [31], last_lr: 0.10000, train_loss: 0.9165, val_loss: 1.3944, val_acc: 0.6316\n",
      "Epoch [32], last_lr: 0.10000, train_loss: 0.9071, val_loss: 1.4057, val_acc: 0.6238\n",
      "Epoch [33], last_lr: 0.10000, train_loss: 0.9120, val_loss: 1.3190, val_acc: 0.6440\n",
      "Epoch [34], last_lr: 0.10000, train_loss: 0.8995, val_loss: 1.3852, val_acc: 0.6344\n",
      "Epoch [35], last_lr: 0.10000, train_loss: 0.8883, val_loss: 1.3065, val_acc: 0.6466\n",
      "Epoch [36], last_lr: 0.10000, train_loss: 0.8773, val_loss: 1.5156, val_acc: 0.6141\n",
      "Epoch [37], last_lr: 0.10000, train_loss: 0.8790, val_loss: 1.3065, val_acc: 0.6394\n",
      "Epoch [38], last_lr: 0.10000, train_loss: 0.8683, val_loss: 1.3671, val_acc: 0.6421\n",
      "Epoch [39], last_lr: 0.10000, train_loss: 0.8547, val_loss: 1.3403, val_acc: 0.6464\n",
      "Epoch [40], last_lr: 0.10000, train_loss: 0.8552, val_loss: 1.3840, val_acc: 0.6335\n",
      "Epoch [41], last_lr: 0.10000, train_loss: 0.8457, val_loss: 1.3220, val_acc: 0.6432\n",
      "Epoch [42], last_lr: 0.10000, train_loss: 0.8448, val_loss: 1.4192, val_acc: 0.6309\n",
      "Epoch [43], last_lr: 0.10000, train_loss: 0.8440, val_loss: 1.4730, val_acc: 0.6200\n",
      "Epoch [44], last_lr: 0.10000, train_loss: 0.8292, val_loss: 1.2799, val_acc: 0.6551\n",
      "Epoch [45], last_lr: 0.10000, train_loss: 0.8347, val_loss: 1.4014, val_acc: 0.6368\n",
      "Epoch [46], last_lr: 0.10000, train_loss: 0.8214, val_loss: 1.4767, val_acc: 0.6290\n",
      "Epoch [47], last_lr: 0.10000, train_loss: 0.8286, val_loss: 1.3929, val_acc: 0.6381\n",
      "Epoch [48], last_lr: 0.10000, train_loss: 0.8200, val_loss: 1.4438, val_acc: 0.6273\n",
      "Epoch [49], last_lr: 0.10000, train_loss: 0.8142, val_loss: 1.3548, val_acc: 0.6431\n",
      "Epoch [50], last_lr: 0.10000, train_loss: 0.8130, val_loss: 1.5372, val_acc: 0.6072\n",
      "Epoch [51], last_lr: 0.10000, train_loss: 0.8098, val_loss: 1.3983, val_acc: 0.6351\n",
      "Epoch [52], last_lr: 0.10000, train_loss: 0.8038, val_loss: 1.4090, val_acc: 0.6321\n",
      "Epoch [53], last_lr: 0.10000, train_loss: 0.8078, val_loss: 1.5214, val_acc: 0.6221\n",
      "Epoch [54], last_lr: 0.10000, train_loss: 0.8027, val_loss: 1.3966, val_acc: 0.6462\n",
      "Epoch [55], last_lr: 0.10000, train_loss: 0.7974, val_loss: 1.3445, val_acc: 0.6454\n",
      "Epoch [56], last_lr: 0.10000, train_loss: 0.7889, val_loss: 1.2547, val_acc: 0.6603\n",
      "Epoch [57], last_lr: 0.10000, train_loss: 0.7840, val_loss: 1.3104, val_acc: 0.6641\n",
      "Epoch [58], last_lr: 0.10000, train_loss: 0.7830, val_loss: 1.4887, val_acc: 0.6240\n",
      "Epoch [59], last_lr: 0.10000, train_loss: 0.7832, val_loss: 1.3213, val_acc: 0.6548\n",
      "Epoch [60], last_lr: 0.10000, train_loss: 0.7722, val_loss: 1.4524, val_acc: 0.6383\n",
      "Epoch [61], last_lr: 0.10000, train_loss: 0.7821, val_loss: 1.3861, val_acc: 0.6350\n",
      "Epoch [62], last_lr: 0.10000, train_loss: 0.7702, val_loss: 1.2694, val_acc: 0.6597\n",
      "Epoch [63], last_lr: 0.10000, train_loss: 0.7768, val_loss: 1.3745, val_acc: 0.6379\n",
      "Epoch [64], last_lr: 0.10000, train_loss: 0.7661, val_loss: 1.3954, val_acc: 0.6443\n",
      "Epoch [65], last_lr: 0.10000, train_loss: 0.7672, val_loss: 1.3713, val_acc: 0.6431\n",
      "Epoch [66], last_lr: 0.10000, train_loss: 0.7651, val_loss: 1.3164, val_acc: 0.6612\n",
      "Epoch [67], last_lr: 0.10000, train_loss: 0.7617, val_loss: 1.4039, val_acc: 0.6394\n",
      "Epoch [68], last_lr: 0.10000, train_loss: 0.7597, val_loss: 1.2753, val_acc: 0.6680\n",
      "Epoch [69], last_lr: 0.10000, train_loss: 0.7513, val_loss: 1.4532, val_acc: 0.6322\n",
      "Epoch [70], last_lr: 0.10000, train_loss: 0.7539, val_loss: 1.3238, val_acc: 0.6586\n",
      "Epoch [71], last_lr: 0.10000, train_loss: 0.7567, val_loss: 1.4811, val_acc: 0.6221\n",
      "Epoch [72], last_lr: 0.10000, train_loss: 0.7509, val_loss: 1.3504, val_acc: 0.6523\n",
      "Epoch [73], last_lr: 0.10000, train_loss: 0.7469, val_loss: 1.2676, val_acc: 0.6673\n",
      "Epoch [74], last_lr: 0.10000, train_loss: 0.7423, val_loss: 1.3487, val_acc: 0.6523\n",
      "Epoch [75], last_lr: 0.01000, train_loss: 0.4184, val_loss: 0.8820, val_acc: 0.7561\n",
      "Epoch [76], last_lr: 0.01000, train_loss: 0.3178, val_loss: 0.8817, val_acc: 0.7622\n",
      "Epoch [77], last_lr: 0.01000, train_loss: 0.2799, val_loss: 0.8871, val_acc: 0.7649\n",
      "Epoch [78], last_lr: 0.01000, train_loss: 0.2504, val_loss: 0.8989, val_acc: 0.7642\n",
      "Epoch [79], last_lr: 0.01000, train_loss: 0.2304, val_loss: 0.9067, val_acc: 0.7651\n",
      "Epoch [80], last_lr: 0.01000, train_loss: 0.2140, val_loss: 0.9159, val_acc: 0.7653\n",
      "Epoch [81], last_lr: 0.01000, train_loss: 0.1995, val_loss: 0.9351, val_acc: 0.7645\n",
      "Epoch [82], last_lr: 0.01000, train_loss: 0.1865, val_loss: 0.9424, val_acc: 0.7651\n",
      "Epoch [83], last_lr: 0.01000, train_loss: 0.1714, val_loss: 0.9560, val_acc: 0.7654\n",
      "Epoch [84], last_lr: 0.01000, train_loss: 0.1627, val_loss: 0.9667, val_acc: 0.7636\n",
      "Epoch [85], last_lr: 0.01000, train_loss: 0.1537, val_loss: 0.9756, val_acc: 0.7654\n",
      "Epoch [86], last_lr: 0.01000, train_loss: 0.1460, val_loss: 0.9921, val_acc: 0.7628\n",
      "Epoch [87], last_lr: 0.01000, train_loss: 0.1366, val_loss: 0.9917, val_acc: 0.7647\n",
      "Epoch [88], last_lr: 0.01000, train_loss: 0.1290, val_loss: 1.0099, val_acc: 0.7623\n",
      "Epoch [89], last_lr: 0.01000, train_loss: 0.1236, val_loss: 1.0222, val_acc: 0.7618\n",
      "Epoch [90], last_lr: 0.01000, train_loss: 0.1179, val_loss: 1.0210, val_acc: 0.7625\n",
      "Epoch [91], last_lr: 0.01000, train_loss: 0.1123, val_loss: 1.0385, val_acc: 0.7629\n",
      "Epoch [92], last_lr: 0.01000, train_loss: 0.1090, val_loss: 1.0514, val_acc: 0.7605\n",
      "Epoch [93], last_lr: 0.01000, train_loss: 0.1029, val_loss: 1.0513, val_acc: 0.7574\n",
      "Epoch [94], last_lr: 0.01000, train_loss: 0.0992, val_loss: 1.0655, val_acc: 0.7553\n",
      "Epoch [95], last_lr: 0.01000, train_loss: 0.0955, val_loss: 1.0677, val_acc: 0.7551\n",
      "Epoch [96], last_lr: 0.01000, train_loss: 0.0905, val_loss: 1.0833, val_acc: 0.7584\n",
      "Epoch [97], last_lr: 0.01000, train_loss: 0.0901, val_loss: 1.0727, val_acc: 0.7587\n",
      "Epoch [98], last_lr: 0.01000, train_loss: 0.0855, val_loss: 1.0814, val_acc: 0.7559\n",
      "Epoch [99], last_lr: 0.01000, train_loss: 0.0823, val_loss: 1.0864, val_acc: 0.7567\n",
      "Epoch [100], last_lr: 0.01000, train_loss: 0.0796, val_loss: 1.0938, val_acc: 0.7599\n",
      "Epoch [101], last_lr: 0.01000, train_loss: 0.0757, val_loss: 1.1135, val_acc: 0.7575\n",
      "Epoch [102], last_lr: 0.01000, train_loss: 0.0755, val_loss: 1.0935, val_acc: 0.7573\n",
      "Epoch [103], last_lr: 0.01000, train_loss: 0.0721, val_loss: 1.1075, val_acc: 0.7601\n",
      "Epoch [104], last_lr: 0.01000, train_loss: 0.0703, val_loss: 1.1337, val_acc: 0.7539\n",
      "Epoch [105], last_lr: 0.01000, train_loss: 0.0711, val_loss: 1.1189, val_acc: 0.7580\n",
      "Epoch [106], last_lr: 0.01000, train_loss: 0.0684, val_loss: 1.1338, val_acc: 0.7578\n",
      "Epoch [107], last_lr: 0.01000, train_loss: 0.0677, val_loss: 1.1361, val_acc: 0.7539\n",
      "Epoch [108], last_lr: 0.01000, train_loss: 0.0667, val_loss: 1.1400, val_acc: 0.7551\n",
      "Epoch [109], last_lr: 0.01000, train_loss: 0.0623, val_loss: 1.1487, val_acc: 0.7537\n",
      "Epoch [110], last_lr: 0.01000, train_loss: 0.0622, val_loss: 1.1364, val_acc: 0.7545\n",
      "Epoch [111], last_lr: 0.01000, train_loss: 0.0613, val_loss: 1.1364, val_acc: 0.7567\n",
      "Epoch [112], last_lr: 0.00100, train_loss: 0.0506, val_loss: 1.1069, val_acc: 0.7635\n",
      "Epoch [113], last_lr: 0.00100, train_loss: 0.0433, val_loss: 1.1071, val_acc: 0.7625\n",
      "Epoch [114], last_lr: 0.00100, train_loss: 0.0393, val_loss: 1.1003, val_acc: 0.7642\n",
      "Epoch [115], last_lr: 0.00100, train_loss: 0.0388, val_loss: 1.1083, val_acc: 0.7620\n",
      "Epoch [116], last_lr: 0.00100, train_loss: 0.0374, val_loss: 1.1097, val_acc: 0.7639\n",
      "Epoch [117], last_lr: 0.00100, train_loss: 0.0368, val_loss: 1.1078, val_acc: 0.7632\n",
      "Epoch [118], last_lr: 0.00100, train_loss: 0.0361, val_loss: 1.1161, val_acc: 0.7640\n",
      "Epoch [119], last_lr: 0.00100, train_loss: 0.0355, val_loss: 1.1112, val_acc: 0.7664\n",
      "Epoch [120], last_lr: 0.00100, train_loss: 0.0353, val_loss: 1.1014, val_acc: 0.7637\n",
      "Epoch [121], last_lr: 0.00100, train_loss: 0.0346, val_loss: 1.1055, val_acc: 0.7657\n",
      "Epoch [122], last_lr: 0.00100, train_loss: 0.0345, val_loss: 1.1057, val_acc: 0.7645\n",
      "Epoch [123], last_lr: 0.00100, train_loss: 0.0344, val_loss: 1.1132, val_acc: 0.7638\n",
      "Epoch [124], last_lr: 0.00100, train_loss: 0.0339, val_loss: 1.1129, val_acc: 0.7623\n",
      "Epoch [125], last_lr: 0.00100, train_loss: 0.0336, val_loss: 1.1112, val_acc: 0.7637\n",
      "Epoch [126], last_lr: 0.00100, train_loss: 0.0326, val_loss: 1.1126, val_acc: 0.7643\n",
      "Epoch [127], last_lr: 0.00100, train_loss: 0.0332, val_loss: 1.1099, val_acc: 0.7634\n",
      "Epoch [128], last_lr: 0.00100, train_loss: 0.0330, val_loss: 1.1179, val_acc: 0.7655\n",
      "Epoch [129], last_lr: 0.00100, train_loss: 0.0327, val_loss: 1.1105, val_acc: 0.7654\n",
      "Epoch [130], last_lr: 0.00100, train_loss: 0.0326, val_loss: 1.1108, val_acc: 0.7640\n",
      "Epoch [131], last_lr: 0.00100, train_loss: 0.0317, val_loss: 1.1136, val_acc: 0.7647\n",
      "Epoch [132], last_lr: 0.00100, train_loss: 0.0317, val_loss: 1.1093, val_acc: 0.7626\n",
      "Epoch [133], last_lr: 0.00100, train_loss: 0.0319, val_loss: 1.1173, val_acc: 0.7650\n",
      "Epoch [134], last_lr: 0.00100, train_loss: 0.0311, val_loss: 1.1150, val_acc: 0.7634\n",
      "Epoch [135], last_lr: 0.00100, train_loss: 0.0316, val_loss: 1.1124, val_acc: 0.7636\n",
      "Epoch [136], last_lr: 0.00100, train_loss: 0.0307, val_loss: 1.1152, val_acc: 0.7633\n",
      "Epoch [137], last_lr: 0.00100, train_loss: 0.0306, val_loss: 1.1166, val_acc: 0.7625\n",
      "Epoch [138], last_lr: 0.00100, train_loss: 0.0310, val_loss: 1.1158, val_acc: 0.7638\n",
      "Epoch [139], last_lr: 0.00100, train_loss: 0.0302, val_loss: 1.1150, val_acc: 0.7635\n",
      "Epoch [140], last_lr: 0.00100, train_loss: 0.0301, val_loss: 1.1216, val_acc: 0.7649\n",
      "Epoch [141], last_lr: 0.00100, train_loss: 0.0297, val_loss: 1.1221, val_acc: 0.7649\n",
      "Epoch [142], last_lr: 0.00100, train_loss: 0.0292, val_loss: 1.1235, val_acc: 0.7639\n",
      "Epoch [143], last_lr: 0.00100, train_loss: 0.0290, val_loss: 1.1159, val_acc: 0.7632\n",
      "Epoch [144], last_lr: 0.00100, train_loss: 0.0296, val_loss: 1.1187, val_acc: 0.7652\n",
      "Epoch [145], last_lr: 0.00100, train_loss: 0.0292, val_loss: 1.1140, val_acc: 0.7634\n",
      "Epoch [146], last_lr: 0.00100, train_loss: 0.0297, val_loss: 1.1285, val_acc: 0.7627\n",
      "Epoch [147], last_lr: 0.00100, train_loss: 0.0290, val_loss: 1.1225, val_acc: 0.7632\n",
      "Epoch [148], last_lr: 0.00100, train_loss: 0.0285, val_loss: 1.1257, val_acc: 0.7648\n",
      "Epoch [149], last_lr: 0.00100, train_loss: 0.0289, val_loss: 1.1145, val_acc: 0.7638\n",
      "CPU times: user 4h 22min 20s, sys: 48min 29s, total: 5h 10min 49s\n",
      "Wall time: 5h 10min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = []\n",
    "history += Train(epochs=epochs,train_dl=train_dl,valid_dl=valid_dl,model=model,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkyW-fhrUZkb"
   },
   "source": [
    "# Model / Parameter statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "6ymMmI0TUvJP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet_Cifar(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(342, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=342, out_features=100, bias=True)\n",
      ")\n",
      "--------------------------------------------------------------------------------\n",
      "        Layer (type)               Output Shape          Params    FLOPs(M+A) #\n",
      "================================================================================\n",
      "            Conv2d-1            [1, 24, 32, 32]             648         1302528\n",
      "       BatchNorm2d-2            [1, 24, 32, 32]              96           49152\n",
      "              ReLU-3            [1, 24, 32, 32]               0               0\n",
      "            Conv2d-4            [1, 48, 32, 32]            1152         2310144\n",
      "       BatchNorm2d-5            [1, 48, 32, 32]             192           98304\n",
      "              ReLU-6            [1, 48, 32, 32]               0               0\n",
      "            Conv2d-7            [1, 12, 32, 32]            5184        10604544\n",
      "       BatchNorm2d-8            [1, 36, 32, 32]             144           73728\n",
      "              ReLU-9            [1, 36, 32, 32]               0               0\n",
      "           Conv2d-10            [1, 48, 32, 32]            1728         3489792\n",
      "      BatchNorm2d-11            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-12            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-13            [1, 12, 32, 32]            5184        10604544\n",
      "      BatchNorm2d-14            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-15            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-16            [1, 48, 32, 32]            2304         4669440\n",
      "      BatchNorm2d-17            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-18            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-19            [1, 12, 32, 32]            5184        10604544\n",
      "      BatchNorm2d-20            [1, 60, 32, 32]             240          122880\n",
      "             ReLU-21            [1, 60, 32, 32]               0               0\n",
      "           Conv2d-22            [1, 48, 32, 32]            2880         5849088\n",
      "      BatchNorm2d-23            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-24            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-25            [1, 12, 32, 32]            5184        10604544\n",
      "      BatchNorm2d-26            [1, 72, 32, 32]             288          147456\n",
      "             ReLU-27            [1, 72, 32, 32]               0               0\n",
      "           Conv2d-28            [1, 48, 32, 32]            3456         7028736\n",
      "      BatchNorm2d-29            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-30            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-31            [1, 12, 32, 32]            5184        10604544\n",
      "      BatchNorm2d-32            [1, 84, 32, 32]             336          172032\n",
      "             ReLU-33            [1, 84, 32, 32]               0               0\n",
      "           Conv2d-34            [1, 48, 32, 32]            4032         8208384\n",
      "      BatchNorm2d-35            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-36            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-37            [1, 12, 32, 32]            5184        10604544\n",
      "      BatchNorm2d-38            [1, 96, 32, 32]             384          196608\n",
      "             ReLU-39            [1, 96, 32, 32]               0               0\n",
      "           Conv2d-40            [1, 48, 32, 32]            4608         9388032\n",
      "      BatchNorm2d-41            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-42            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-43            [1, 12, 32, 32]            5184        10604544\n",
      "      BatchNorm2d-44           [1, 108, 32, 32]             432          221184\n",
      "             ReLU-45           [1, 108, 32, 32]               0               0\n",
      "           Conv2d-46            [1, 48, 32, 32]            5184        10567680\n",
      "      BatchNorm2d-47            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-48            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-49            [1, 12, 32, 32]            5184        10604544\n",
      "      BatchNorm2d-50           [1, 120, 32, 32]             480          245760\n",
      "             ReLU-51           [1, 120, 32, 32]               0               0\n",
      "           Conv2d-52            [1, 48, 32, 32]            5760        11747328\n",
      "      BatchNorm2d-53            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-54            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-55            [1, 12, 32, 32]            5184        10604544\n",
      "      BatchNorm2d-56           [1, 132, 32, 32]             528          270336\n",
      "             ReLU-57           [1, 132, 32, 32]               0               0\n",
      "           Conv2d-58            [1, 48, 32, 32]            6336        12926976\n",
      "      BatchNorm2d-59            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-60            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-61            [1, 12, 32, 32]            5184        10604544\n",
      "      BatchNorm2d-62           [1, 144, 32, 32]             576          294912\n",
      "             ReLU-63           [1, 144, 32, 32]               0               0\n",
      "           Conv2d-64            [1, 48, 32, 32]            6912        14106624\n",
      "      BatchNorm2d-65            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-66            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-67            [1, 12, 32, 32]            5184        10604544\n",
      "      BatchNorm2d-68           [1, 156, 32, 32]             624          319488\n",
      "             ReLU-69           [1, 156, 32, 32]               0               0\n",
      "           Conv2d-70            [1, 48, 32, 32]            7488        15286272\n",
      "      BatchNorm2d-71            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-72            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-73            [1, 12, 32, 32]            5184        10604544\n",
      "      BatchNorm2d-74           [1, 168, 32, 32]             672          344064\n",
      "             ReLU-75           [1, 168, 32, 32]               0               0\n",
      "           Conv2d-76            [1, 48, 32, 32]            8064        16465920\n",
      "      BatchNorm2d-77            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-78            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-79            [1, 12, 32, 32]            5184        10604544\n",
      "      BatchNorm2d-80           [1, 180, 32, 32]             720          368640\n",
      "             ReLU-81           [1, 180, 32, 32]               0               0\n",
      "           Conv2d-82            [1, 48, 32, 32]            8640        17645568\n",
      "      BatchNorm2d-83            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-84            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-85            [1, 12, 32, 32]            5184        10604544\n",
      "      BatchNorm2d-86           [1, 192, 32, 32]             768          393216\n",
      "             ReLU-87           [1, 192, 32, 32]               0               0\n",
      "           Conv2d-88            [1, 48, 32, 32]            9216        18825216\n",
      "      BatchNorm2d-89            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-90            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-91            [1, 12, 32, 32]            5184        10604544\n",
      "      BatchNorm2d-92           [1, 204, 32, 32]             816          417792\n",
      "             ReLU-93           [1, 204, 32, 32]               0               0\n",
      "           Conv2d-94            [1, 48, 32, 32]            9792        20004864\n",
      "      BatchNorm2d-95            [1, 48, 32, 32]             192           98304\n",
      "             ReLU-96            [1, 48, 32, 32]               0               0\n",
      "           Conv2d-97            [1, 12, 32, 32]            5184        10604544\n",
      "      BatchNorm2d-98           [1, 216, 32, 32]             864          442368\n",
      "             ReLU-99           [1, 216, 32, 32]               0               0\n",
      "          Conv2d-100           [1, 108, 32, 32]           23328        47665152\n",
      "       AvgPool2d-101           [1, 108, 16, 16]               0               0\n",
      "     BatchNorm2d-102           [1, 108, 16, 16]             432           55296\n",
      "            ReLU-103           [1, 108, 16, 16]               0               0\n",
      "          Conv2d-104            [1, 48, 16, 16]            5184         2641920\n",
      "     BatchNorm2d-105            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-106            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-107            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-108           [1, 120, 16, 16]             480           61440\n",
      "            ReLU-109           [1, 120, 16, 16]               0               0\n",
      "          Conv2d-110            [1, 48, 16, 16]            5760         2936832\n",
      "     BatchNorm2d-111            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-112            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-113            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-114           [1, 132, 16, 16]             528           67584\n",
      "            ReLU-115           [1, 132, 16, 16]               0               0\n",
      "          Conv2d-116            [1, 48, 16, 16]            6336         3231744\n",
      "     BatchNorm2d-117            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-118            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-119            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-120           [1, 144, 16, 16]             576           73728\n",
      "            ReLU-121           [1, 144, 16, 16]               0               0\n",
      "          Conv2d-122            [1, 48, 16, 16]            6912         3526656\n",
      "     BatchNorm2d-123            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-124            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-125            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-126           [1, 156, 16, 16]             624           79872\n",
      "            ReLU-127           [1, 156, 16, 16]               0               0\n",
      "          Conv2d-128            [1, 48, 16, 16]            7488         3821568\n",
      "     BatchNorm2d-129            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-130            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-131            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-132           [1, 168, 16, 16]             672           86016\n",
      "            ReLU-133           [1, 168, 16, 16]               0               0\n",
      "          Conv2d-134            [1, 48, 16, 16]            8064         4116480\n",
      "     BatchNorm2d-135            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-136            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-137            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-138           [1, 180, 16, 16]             720           92160\n",
      "            ReLU-139           [1, 180, 16, 16]               0               0\n",
      "          Conv2d-140            [1, 48, 16, 16]            8640         4411392\n",
      "     BatchNorm2d-141            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-142            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-143            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-144           [1, 192, 16, 16]             768           98304\n",
      "            ReLU-145           [1, 192, 16, 16]               0               0\n",
      "          Conv2d-146            [1, 48, 16, 16]            9216         4706304\n",
      "     BatchNorm2d-147            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-148            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-149            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-150           [1, 204, 16, 16]             816          104448\n",
      "            ReLU-151           [1, 204, 16, 16]               0               0\n",
      "          Conv2d-152            [1, 48, 16, 16]            9792         5001216\n",
      "     BatchNorm2d-153            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-154            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-155            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-156           [1, 216, 16, 16]             864          110592\n",
      "            ReLU-157           [1, 216, 16, 16]               0               0\n",
      "          Conv2d-158            [1, 48, 16, 16]           10368         5296128\n",
      "     BatchNorm2d-159            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-160            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-161            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-162           [1, 228, 16, 16]             912          116736\n",
      "            ReLU-163           [1, 228, 16, 16]               0               0\n",
      "          Conv2d-164            [1, 48, 16, 16]           10944         5591040\n",
      "     BatchNorm2d-165            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-166            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-167            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-168           [1, 240, 16, 16]             960          122880\n",
      "            ReLU-169           [1, 240, 16, 16]               0               0\n",
      "          Conv2d-170            [1, 48, 16, 16]           11520         5885952\n",
      "     BatchNorm2d-171            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-172            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-173            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-174           [1, 252, 16, 16]            1008          129024\n",
      "            ReLU-175           [1, 252, 16, 16]               0               0\n",
      "          Conv2d-176            [1, 48, 16, 16]           12096         6180864\n",
      "     BatchNorm2d-177            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-178            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-179            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-180           [1, 264, 16, 16]            1056          135168\n",
      "            ReLU-181           [1, 264, 16, 16]               0               0\n",
      "          Conv2d-182            [1, 48, 16, 16]           12672         6475776\n",
      "     BatchNorm2d-183            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-184            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-185            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-186           [1, 276, 16, 16]            1104          141312\n",
      "            ReLU-187           [1, 276, 16, 16]               0               0\n",
      "          Conv2d-188            [1, 48, 16, 16]           13248         6770688\n",
      "     BatchNorm2d-189            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-190            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-191            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-192           [1, 288, 16, 16]            1152          147456\n",
      "            ReLU-193           [1, 288, 16, 16]               0               0\n",
      "          Conv2d-194            [1, 48, 16, 16]           13824         7065600\n",
      "     BatchNorm2d-195            [1, 48, 16, 16]             192           24576\n",
      "            ReLU-196            [1, 48, 16, 16]               0               0\n",
      "          Conv2d-197            [1, 12, 16, 16]            5184         2651136\n",
      "     BatchNorm2d-198           [1, 300, 16, 16]            1200          153600\n",
      "            ReLU-199           [1, 300, 16, 16]               0               0\n",
      "          Conv2d-200           [1, 150, 16, 16]           45000        23001600\n",
      "       AvgPool2d-201             [1, 150, 8, 8]               0               0\n",
      "     BatchNorm2d-202             [1, 150, 8, 8]             600           19200\n",
      "            ReLU-203             [1, 150, 8, 8]               0               0\n",
      "          Conv2d-204              [1, 48, 8, 8]            7200          918528\n",
      "     BatchNorm2d-205              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-206              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-207              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-208             [1, 162, 8, 8]             648           20736\n",
      "            ReLU-209             [1, 162, 8, 8]               0               0\n",
      "          Conv2d-210              [1, 48, 8, 8]            7776          992256\n",
      "     BatchNorm2d-211              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-212              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-213              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-214             [1, 174, 8, 8]             696           22272\n",
      "            ReLU-215             [1, 174, 8, 8]               0               0\n",
      "          Conv2d-216              [1, 48, 8, 8]            8352         1065984\n",
      "     BatchNorm2d-217              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-218              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-219              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-220             [1, 186, 8, 8]             744           23808\n",
      "            ReLU-221             [1, 186, 8, 8]               0               0\n",
      "          Conv2d-222              [1, 48, 8, 8]            8928         1139712\n",
      "     BatchNorm2d-223              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-224              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-225              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-226             [1, 198, 8, 8]             792           25344\n",
      "            ReLU-227             [1, 198, 8, 8]               0               0\n",
      "          Conv2d-228              [1, 48, 8, 8]            9504         1213440\n",
      "     BatchNorm2d-229              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-230              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-231              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-232             [1, 210, 8, 8]             840           26880\n",
      "            ReLU-233             [1, 210, 8, 8]               0               0\n",
      "          Conv2d-234              [1, 48, 8, 8]           10080         1287168\n",
      "     BatchNorm2d-235              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-236              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-237              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-238             [1, 222, 8, 8]             888           28416\n",
      "            ReLU-239             [1, 222, 8, 8]               0               0\n",
      "          Conv2d-240              [1, 48, 8, 8]           10656         1360896\n",
      "     BatchNorm2d-241              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-242              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-243              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-244             [1, 234, 8, 8]             936           29952\n",
      "            ReLU-245             [1, 234, 8, 8]               0               0\n",
      "          Conv2d-246              [1, 48, 8, 8]           11232         1434624\n",
      "     BatchNorm2d-247              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-248              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-249              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-250             [1, 246, 8, 8]             984           31488\n",
      "            ReLU-251             [1, 246, 8, 8]               0               0\n",
      "          Conv2d-252              [1, 48, 8, 8]           11808         1508352\n",
      "     BatchNorm2d-253              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-254              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-255              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-256             [1, 258, 8, 8]            1032           33024\n",
      "            ReLU-257             [1, 258, 8, 8]               0               0\n",
      "          Conv2d-258              [1, 48, 8, 8]           12384         1582080\n",
      "     BatchNorm2d-259              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-260              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-261              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-262             [1, 270, 8, 8]            1080           34560\n",
      "            ReLU-263             [1, 270, 8, 8]               0               0\n",
      "          Conv2d-264              [1, 48, 8, 8]           12960         1655808\n",
      "     BatchNorm2d-265              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-266              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-267              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-268             [1, 282, 8, 8]            1128           36096\n",
      "            ReLU-269             [1, 282, 8, 8]               0               0\n",
      "          Conv2d-270              [1, 48, 8, 8]           13536         1729536\n",
      "     BatchNorm2d-271              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-272              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-273              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-274             [1, 294, 8, 8]            1176           37632\n",
      "            ReLU-275             [1, 294, 8, 8]               0               0\n",
      "          Conv2d-276              [1, 48, 8, 8]           14112         1803264\n",
      "     BatchNorm2d-277              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-278              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-279              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-280             [1, 306, 8, 8]            1224           39168\n",
      "            ReLU-281             [1, 306, 8, 8]               0               0\n",
      "          Conv2d-282              [1, 48, 8, 8]           14688         1876992\n",
      "     BatchNorm2d-283              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-284              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-285              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-286             [1, 318, 8, 8]            1272           40704\n",
      "            ReLU-287             [1, 318, 8, 8]               0               0\n",
      "          Conv2d-288              [1, 48, 8, 8]           15264         1950720\n",
      "     BatchNorm2d-289              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-290              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-291              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-292             [1, 330, 8, 8]            1320           42240\n",
      "            ReLU-293             [1, 330, 8, 8]               0               0\n",
      "          Conv2d-294              [1, 48, 8, 8]           15840         2024448\n",
      "     BatchNorm2d-295              [1, 48, 8, 8]             192            6144\n",
      "            ReLU-296              [1, 48, 8, 8]               0               0\n",
      "          Conv2d-297              [1, 12, 8, 8]            5184          662784\n",
      "     BatchNorm2d-298             [1, 342, 8, 8]            1368           43776\n",
      "          Linear-299                   [1, 100]           34300           68400\n",
      "  DenseNet_Cifar-300                   [1, 100]               0               0\n",
      "================================================================================\n",
      "        Total parameters: 824,020  824.0K\n",
      "    Trainable parameters: 800,032\n",
      "Non-trainable parameters: 23,988\n",
      "Total flops(M)  : 291,505,176  291.5M\n",
      "Total flops(M+A): 583,010,352  583.0M\n",
      "--------------------------------------------------------------------------------\n",
      "Parameters size (MB): 3.14\n"
     ]
    }
   ],
   "source": [
    "# Print model\n",
    "print(model)\n",
    "\n",
    "# Print parameter\n",
    "net = densenet_BC_cifar(100, 12, num_classes=100)\n",
    "input = torch.randn(1, 3, 32, 32)\n",
    "summary(net,input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpLDHxZaR29V"
   },
   "source": [
    "# Plot Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "r3Hya-vRSDCd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot([x.get(\"train_loss\") for x in history], \"-bx\")\n",
    "    plt.plot([x[\"val_loss\"] for x in history],\"-rx\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"train loss\",\"val loss\"])\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uc0SOdZvmWeX"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "tlGEEjYcY2yf"
   },
   "outputs": [],
   "source": [
    "def test(model,data_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        correct = 0\n",
    "        bs = 100\n",
    "        result = []\n",
    "        check_names = []\n",
    "        for i, (data, target) in enumerate(valid_dl):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            arr = pred.data.cpu().numpy()\n",
    "            for j in range(pred.size()[0]):\n",
    "                file_name = valid_ds.samples[i*bs+j][0].split('/')[-1]\n",
    "                result.append((file_name,pred[j].cpu().numpy()[0])) \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "w4Nsco9zZJX5"
   },
   "outputs": [],
   "source": [
    "result = test(model, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DiZ2EDCrFXP"
   },
   "source": [
    "# Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "thLeLAP6rK-X"
   },
   "outputs": [],
   "source": [
    "with open ('ID_result.csv','w') as f:\n",
    "    f.write('Id,Category\\n')\n",
    "    for data in result:\n",
    "        f.write(data[0]+','+str(data[1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'midterm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "test2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
